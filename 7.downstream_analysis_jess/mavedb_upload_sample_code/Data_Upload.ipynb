{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8bae9bc-8f9f-4c32-aabf-da3c94b37a72",
   "metadata": {},
   "source": [
    "# Upload VarChAMP data to MaveDB\n",
    "Jess Ewald (modified from notebook by Alan Rubin)\n",
    "2024-11-22\n",
    "\n",
    "This notebook uploads VarChAMP experiments and score sets from the imaging work to [MaveDB](https://www.mavedb.org/) using the Python API client implemented in [mavetools](https://github.com/VariantEffect/mavetools). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "62234585-4bb8-4f71-ba5d-2cb1f1900846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import urllib.request\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import requests\n",
    "from fqfa.util.translate import translate_dna\n",
    "from mavehgvs import Variant\n",
    "from mavedb import __version__ as mavedb_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273edef-46db-44d1-af97-46cc9e1fd641",
   "metadata": {},
   "source": [
    "## Set up API key and endpoint\n",
    "\n",
    "You can view your API key by logging into MaveDB and then visiting the [settings](https://mavedb.org/#/settings) page.\n",
    "Copy the API key here, as this is required by the client to create records and view your private records.\n",
    "You can also set up the API key using an environment variable `MAVEDB_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f6edcd50-60c1-4e02-92c8-557b3ea038bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"MAVEDB_API_KEY\" in os.environ:\n",
    "    api_key = os.environ.get(\"MAVEDB_API_KEY\")\n",
    "else:\n",
    "    api_key = \"CHkJSgtKgNs7TxxP2-vWQiEsbhl8yJLOgTcMQ0TIV0Y\"\n",
    "\n",
    "# API URL for the production MaveDB instance\n",
    "api_url = \"https://api.mavedb.org/api/v1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0690ea0-d23a-476a-a993-f9cad1f07fb1",
   "metadata": {},
   "source": [
    "If you are having problems with validation, compare the version of the MaveDB data models mavetools is using with the version of MaveDB running on the server you are accessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47e571db-856e-4205-9529-6f21cca899e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API version:       2024.4.2\n",
      "Module version:    2024.4.1\n"
     ]
    }
   ],
   "source": [
    "with urllib.request.urlopen(f\"{api_url}api/version\") as response:\n",
    "    r = response.read()\n",
    "    print(f\"API version:{json.loads(r)['version']:>15}\")\n",
    "print(f\"Module version:{mavedb_version:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2863bc6e-6c5f-498d-8676-86b52c36a9d9",
   "metadata": {},
   "source": [
    "## Format the input data\n",
    "\n",
    "MaveDB requires data in specific formats, including precisely formatted identifiers and column names. Key changes here are the creation of the \"hgvs_pro\" column, appending the Ensembl IDs (without the version number), adding the target sequences, and reformatting the variant nucleotide changes. Each uploaded score_set file must have a \"score\" column. Also, target (gene) labels may not have any spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format allele file\n",
    "dat_info = pl.read_csv(\"./jess_allles_with_mutated_cds.tsv\", separator=\"\\t\").with_columns(\n",
    "    pl.col(\"variant\").str.replace(\"_.*\", \"\").alias(\"symbol\")\n",
    ")\n",
    "\n",
    "dat_info = dat_info.with_columns(\n",
    "    pl.col(\"symbol\").str.replace(\" \", \"_\").alias(\"symbol\")\n",
    ")\n",
    "\n",
    "dat_info = dat_info.with_columns(\n",
    "    pl.concat_str([\n",
    "        pl.col(\"symbol\"),\n",
    "        pl.lit(\":n.\"),\n",
    "        pl.col(\"nt_change\")\n",
    "    ], separator=\"\").alias(\"hgvs_nt\"),\n",
    "    pl.col(\"ensembl_protein_id\").str.replace(r\"\\..*\", \"\", literal=False).alias(\"ensembl_protein_id\")\n",
    ").rename({\"variant\": \"Variant\"})\n",
    "\n",
    "# get only gene-level info\n",
    "gene_info = dat_info.select(\n",
    "    [\"symbol\", \"ensembl_protein_id\", \"ref_cds\"]\n",
    ").unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1932ebc",
   "metadata": {},
   "source": [
    "We decided that there will be a separate scoreSet for each measurement, so here we keep the localization and abundance scores separate. Each table must have an \"hgvs_nt\" column and a \"score\" column. There can be additional columns that provide complementary stats for the \"score\" (ie. p-value, confidence interval, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "58a5243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format abundance and localization data\n",
    "var_info = dat_info.select([\n",
    "    \"Variant\", \"hgvs_nt\"\n",
    "])\n",
    "\n",
    "# Reformat localization\n",
    "local = pl.read_csv(\"./varchamp_data/1_auroc.csv\").join(\n",
    "    var_info, on=\"Variant\"\n",
    ").select([\"hgvs_nt\", \"mean_auroc\", \"Mislocalized_both_batches\"]).rename({\n",
    "    \"Mislocalized_both_batches\": \"Mislocalization_hit\",\n",
    "    \"mean_auroc\": \"score\",\n",
    "    })\n",
    "\n",
    "# Reformat abundance\n",
    "abun = pl.read_csv(\"./varchamp_data/2_abundance_changes.csv\").join(\n",
    "    var_info, on=\"Variant\"\n",
    ").select([\"hgvs_nt\", \"U2OS_Z\"]).rename({\"U2OS_Z\": \"score\"})\n",
    "\n",
    "# write out scores\n",
    "local.write_csv(\"./varchamp_data/localization_scores.csv\")\n",
    "abun.write_csv(\"./varchamp_data/abundance_scores.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3e01b",
   "metadata": {},
   "source": [
    "## Format experiment and dataset entries\n",
    "\n",
    "MaveDB requires several pieces of text metadata for each record (see the [upload guide](https://www.mavedb.org/docs/mavedb/upload_guide.html)). These functions populate all of the key fields required to characterize the VarChAMP data. We decided that there will be one \"experimentSet\" for each large batch of submitted data. The \"method_text\" field in format_experiment function describes the basic wet lab protocol used to generate the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3d8b3ae7-a44c-4e17-91c6-a3b8062c8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_experiment(experiment_set_urn=None):\n",
    "\n",
    "    dataset = {\n",
    "        \"title\" : \"VarChAMP_Imaging_November_2024\",\n",
    "        \"short_description\" : \"Protein localization and abundance from images of cells.\",\n",
    "        \"abstract_text\" : \"This study measured protein subcellular localization and abundance using fluorescence microscopy.\",\n",
    "        \"method_text\" : \"Entry clones of alleles were transferred using Gateway technology into a mammalian expression pLenti6.2 plasmid containing a C-terminal mNeonGreen fusion (plasmid modified from Addgene 87075). Inserts were verified by restriction digestion and clones that did not produce the expected digestion pattern were omitted from further analysis. Lentiviral constructs were packaged in HEK 293T cells seeded in 96-well plates, then viral supernatant was transferred to spinfect U2OS cells seeded in 384-well plates (4x technical replicates were performed by administering the same viral supernatant to 4 different wells, all viral production and infection was repeated for on a separate day 2x biological replicates). 48 hrs following infection, cells were selected for infection and protein overexpression by applying puromycin for 48 hrs. Cells were then stained with 500 nM MitoTracker Deep Red 1 hr prior to paraformaldehyde fixation. Blocking, permeabilization and staining (8.25 nM Alexa Fluorâ„¢ 568 Phalloidin, 1 ug/mL Hoechst 33342, 1.5 ug/mL WGA Alexa Fluor 555) was then performed in one step. All confocal images were captured on a Perkin Elmer Opera Phenix Microscope (20X water objective, 384 wells, 9 fields).\",\n",
    "        \"extra_metadata\" : {},\n",
    "        \"primary_publication_identifiers\" : [],\n",
    "        \"raw_read_identifiers\" : [],\n",
    "    }\n",
    "    if experiment_set_urn:  # add to an existing experiment set\n",
    "        dataset[\"experiment_set_urn\"] = experiment_set_urn\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62031805",
   "metadata": {},
   "source": [
    "The next two functions format each of the scoreSet submissions. Here, the \"method_text\" describes the data processing pipeline used to compute the submitted scores. The \"label\" field for each target_sequence must match the hgvs_nt prefix for the variants to map properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2bf20-e1b7-4522-8b06-30bba3cecfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_localization_score_set(gene_info, experiment_urn):\n",
    "    target_genes = [\n",
    "        {\n",
    "            \"name\": row[\"symbol\"],\n",
    "            \"category\": \"protein_coding\",\n",
    "            \"external_identifiers\": [\n",
    "                {\n",
    "                    \"identifier\": {\n",
    "                        \"dbName\": \"Ensembl\",\n",
    "                        \"identifier\": row[\"ensembl_protein_id\"]\n",
    "                    },\n",
    "                    'offset': 0,\n",
    "                },\n",
    "            ],\n",
    "            \"target_sequence\": {\n",
    "                \"sequence\": row[\"ref_cds\"],\n",
    "                \"sequence_type\": \"dna\",\n",
    "                \"taxonomy\": {\n",
    "                    \"tax_id\": 9606,\n",
    "                },\n",
    "                \"label\": row[\"symbol\"] # THIS MUST MATCH THE PREFIX IN THE hgvs_nt COLUMN OF THE SCORE SET\n",
    "            },\n",
    "        }\n",
    "        for row in gene_info.to_dicts()\n",
    "    ]\n",
    "\n",
    "    dataset = {\n",
    "        \"title\": \"VarChAMP_Imaging_Localization_November_2024\",\n",
    "        \"short_description\": \"Protein localization from images of cells.\",\n",
    "        \"abstract_text\": (\n",
    "            \"This study measured protein subcellular localization using fluorescence microscopy.\"\n",
    "        ),\n",
    "        \"method_text\": (\n",
    "            \"We used CellProfiler to create morphological profiles of single cells using images from the protein channel (GFP). \"\n",
    "            \"Profiles were filtered to remove features with low variance or missing values, and were MAD-normalized within each plate. \"\n",
    "            \"Cells with abnormal cytoplasm:nucleoplasm area ratios or with median GFP intensities > 5 MAD from the median were filtered out. \"\n",
    "            \"A binary XGBoost classifier was trained to distinguish single-cell profiles for each reference-variant pair, with 4-fold cross-validation and data splits by plate. \"\n",
    "            \"Binary XGBoost classifiers were also trained between all possible pairs of control wells that were repeated on each plate, to quantify the well position effect. \"\n",
    "            \"Reference-variant classifier AUROC values were compared to the technical well position null AUROC values to determine which ones showed evidence from differences in the protein channel that exceeded technical artifacts. \"\n",
    "            \"These 'hits' were considered variants that cause protein mislocalization. \"\n",
    "        ),\n",
    "        \"extra_metadata\": {},\n",
    "        \"primary_publication_identifiers\": [],\n",
    "        \"experiment_urn\": experiment_urn,\n",
    "        \"license_id\": 1,\n",
    "        \"target_genes\": target_genes,\n",
    "    }\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_abundance_score_set(gene_info, experiment_urn):\n",
    "    target_genes = [\n",
    "        {\n",
    "            \"name\": row[\"symbol\"],\n",
    "            \"category\": \"protein_coding\",\n",
    "            \"external_identifiers\": [\n",
    "                {\n",
    "                    \"identifier\": {\n",
    "                        \"dbName\": \"Ensembl\",\n",
    "                        \"identifier\": row[\"ensembl_protein_id\"]\n",
    "                    },\n",
    "                    'offset': 0,\n",
    "                },\n",
    "            ],\n",
    "            \"target_sequence\": {\n",
    "                \"sequence\": row[\"ref_cds\"],\n",
    "                \"sequence_type\": \"dna\",\n",
    "                \"taxonomy\": {\n",
    "                    \"tax_id\": 9606,\n",
    "                },\n",
    "                \"label\": row[\"symbol\"] # THIS MUST MATCH THE PREFIX IN THE hgvs_nt COLUMN OF THE SCORE SET\n",
    "            },\n",
    "        }\n",
    "        for row in gene_info.to_dicts()\n",
    "    ]\n",
    "\n",
    "    dataset = {\n",
    "        \"title\": \"VarChAMP_Imaging_Abundance_November_2024\",\n",
    "        \"short_description\": \"Protein abundance from images of cells.\",\n",
    "        \"abstract_text\": (\n",
    "            \"This study measured protein subcellular abundance using fluorescence microscopy.\"\n",
    "        ),\n",
    "        \"method_text\": (\n",
    "            \"We used CellProfiler to create morphological profiles of single cells using images from the protein channel (GFP). \"\n",
    "            \"Profiles were filtered to remove features with low variance or missing values, and were MAD-normalized within each plate. \"\n",
    "            \"Cells with abnormal cytoplasm:nucleoplasm area ratios or with median GFP intensities > 5 MAD from the median were filtered out. \"\n",
    "            \"We measured changes in protein abundance across reference-variant pairs by computing differences in median protein intensity, while controlling for plate as a random effect.\"\n",
    "        ),\n",
    "        \"extra_metadata\": {},\n",
    "        \"primary_publication_identifiers\": [],\n",
    "        \"experiment_urn\": experiment_urn,\n",
    "        \"license_id\": 1,\n",
    "        \"target_genes\": target_genes,\n",
    "    }\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf52ea",
   "metadata": {},
   "source": [
    "## Upload the experiment\n",
    "\n",
    "The next few steps will upload the data to maveDB. When developing this, I made many errors, resulting in half-completed submissions. If you log into your MaveDB account and go to the dashboard, you can see a record of all of your uploaded experiments and their associated scores. From this interface, you can delete submissions which is helpful if you need to start over again.  \n",
    "\n",
    "The first submission creates an experiment \"urn\" ID - this is like creating an experiment folder in your maveDB account. Knowing the IDs is useful, because you can append additional submissions to previously created experiments. There is no need to manually track these IDs - they are available on your maveDB online dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded experiment:\ttmp:d991ec9d-8f02-48ac-a9e0-312aeb1d8c06\n"
     ]
    }
   ],
   "source": [
    "# Upload to maveDB\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "temp_datasets = list()\n",
    "\n",
    "# upload experimentSet info\n",
    "response = requests.post(\n",
    "    api_url+'experiments/',\n",
    "    json=format_experiment(),\n",
    "    headers={\"X-API-Key\": api_key}\n",
    ")\n",
    "response_data = response.json()\n",
    "created_exp = response_data[\"urn\"]\n",
    "print(f\"uploaded experiment:\\t{created_exp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e73f5",
   "metadata": {},
   "source": [
    "## Upload the mislocalization score set\n",
    "\n",
    "To submit scoreSets to an experimentSet, we must pass in the experiment urn ID (\"created_exp\") as a parameter to the API request. Each experiment set also returns an urn ID (\"created_ss\"). We use this scoreSet ID to post the actual table with all of the scores (ie. localization_scores.csv) to the correct scoreSet description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e3d7b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded score set:\ttmp:8e561ceb-f17d-41ce-a746-a21abfeec2ea\n",
      "uploaded scores for score set:\ttmp:8e561ceb-f17d-41ce-a746-a21abfeec2ea\n",
      "elapsed time:\t4.52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# upload dataSet info\n",
    "response = requests.post(\n",
    "    api_url+\"score-sets/\",\n",
    "    json={**format_localization_score_set(gene_info, experiment_urn=created_exp)},\n",
    "    headers={\"X-API-Key\": api_key}\n",
    ")\n",
    "response_data = response.json()\n",
    "created_ss = response_data[\"urn\"]\n",
    "print(f\"uploaded score set:\\t{created_ss}\")\n",
    "\n",
    "# upload scores file\n",
    "response = requests.post(\n",
    "    api_url+f\"score-sets/{created_ss}/variants/data\",\n",
    "    files={\n",
    "        \"scores_file\": (\"scores.csv\", pd.read_csv(f\"/Users/jewald/Desktop/mavedb_upload_sample_code/varchamp_data/localization_scores.csv\").to_csv()),\n",
    "    },\n",
    "    headers={\"X-API-Key\": api_key}\n",
    ")\n",
    "response.raise_for_status()\n",
    "print(f\"uploaded scores for score set:\\t{created_ss}\")\n",
    "\n",
    "# finish up\n",
    "end = timer()\n",
    "print(f\"elapsed time:\\t{end - start:.2f}\", end=\"\\n\\n\")\n",
    "\n",
    "temp_datasets.append(created_ss)\n",
    "\n",
    "with open(\"temp_accessions.txt\", \"w\") as handle:\n",
    "    for urn_ss in temp_datasets:\n",
    "        print(urn_ss, file=handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb2c84",
   "metadata": {},
   "source": [
    "## Upload the abundance scores\n",
    "\n",
    "Now we repeat the process for the abundance scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "84458e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded score set:\ttmp:af037e24-b2a8-4ccf-898c-031ceed49299\n",
      "uploaded scores for score set:\ttmp:af037e24-b2a8-4ccf-898c-031ceed49299\n",
      "elapsed time:\t8.08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# upload dataSet info\n",
    "response = requests.post(\n",
    "    api_url+\"score-sets/\",\n",
    "    json={**format_abundance_score_set(gene_info, experiment_urn=created_exp)},\n",
    "    headers={\"X-API-Key\": api_key}\n",
    ")\n",
    "response_data = response.json()\n",
    "created_ss = response_data[\"urn\"]\n",
    "print(f\"uploaded score set:\\t{created_ss}\")\n",
    "\n",
    "# upload scores file\n",
    "response = requests.post(\n",
    "    api_url+f\"score-sets/{created_ss}/variants/data\",\n",
    "    files={\n",
    "        \"scores_file\": (\"scores.csv\", pd.read_csv(f\"/Users/jewald/Desktop/mavedb_upload_sample_code/varchamp_data/abundance_scores.csv\").to_csv()),\n",
    "    },\n",
    "    headers={\"X-API-Key\": api_key}\n",
    ")\n",
    "response.raise_for_status()\n",
    "print(f\"uploaded scores for score set:\\t{created_ss}\")\n",
    "\n",
    "# finish up\n",
    "end = timer()\n",
    "print(f\"elapsed time:\\t{end - start:.2f}\", end=\"\\n\\n\")\n",
    "\n",
    "temp_datasets.append(created_ss)\n",
    "\n",
    "with open(\"temp_accessions.txt\", \"w\") as handle:\n",
    "    for urn_ss in temp_datasets:\n",
    "        print(urn_ss, file=handle)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randompython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
